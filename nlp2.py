# -*- coding: utf-8 -*-
"""NLP2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14FmIDIXXpzDKc2Mjchiy0KFVNj7ZqN-7

# Patient Condition Classification Using Drug Reviews

## Objective  
Classify patient conditions (Depression, High Blood Pressure, Type 2 Diabetes)  
using NLP on patient drug reviews to support healthcare insights and recommendations.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import warnings

warnings.filterwarnings("ignore")

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')

# Replace with the full path to your file
df = pd.read_excel("drugsCom_raw.xlsx")
df.head()

# Dataset Overview
print(df.shape)
print("===" * 40)
print(df.info())
print("===" * 40)
print(df.describe())

# Check for Missing Values
print(df.isnull().sum())
print("===" * 40)
print(df.duplicated().sum())

# fix missing values
df['condition'] = df['condition'].fillna(df['condition'].mode()[0])
df.isnull().sum()

# Target Condition Selection
target_conditions = [
    "Depression",
    "High Blood Pressure",
    "Diabetes, Type 2"
]

df = df[df["condition"].isin(target_conditions)]

df["condition"].value_counts()

# Class Distribution Visualization
plt.figure(figsize=(6,4))
sns.countplot(x="condition", data=df)
plt.title("Condition Distribution")
plt.xticks(rotation=30)
plt.show()

# Rating Distribution
plt.figure(figsize=(6,4))
sns.histplot(df["rating"], bins=10, kde=True)
plt.title("Rating Distribution")
plt.show()

# Review Length Analysis
df["review_length"] = df["review"].apply(len)

plt.figure(figsize=(6,4))
sns.histplot(df["review_length"], bins=50)
plt.title("Review Length Distribution")
plt.show()

def label_sentiment(r):
    if r >= 7:
        return 'positive'
    elif r <= 4:
        return 'negative'
    else:
        return 'neutral'

df['sentiment'] = df['rating'].apply(label_sentiment)
df['sentiment'].value_counts(normalize=True) * 100

df['date'] = pd.to_datetime(df['date'], errors='coerce')
df.groupby(df['date'].dt.year)['rating'].mean().plot()
plt.title('Average Rating Over Time')
plt.show()

# Word Count Analysis
df["word_count"] = df["review"].apply(lambda x: len(str(x).split()))

df["word_count"].describe()

# Sample Reviews Per Condition
for condition in target_conditions:
    print(f"\n--- {condition} ---")
    print(df[df["condition"] == condition]["review"].iloc[0][:500])

"""### PREPROCESSING SECTION"""

# Text Cleaning Function
def clean_text(text):
    text = text.lower()
    text = re.sub(r"<.*?>", "", text)        # Remove HTML
    text = re.sub(r"[^a-zA-Z\s]", "", text) # Remove special chars
    text = re.sub(r"\s+", " ", text).strip()
    return text

# Apply Cleaning
df["clean_review"] = df["review"].apply(clean_text)

df[["review", "clean_review"]].head()

import nltk
nltk.download('punkt_tab')

# Apply Preprocessing
def preprocess_text(text):
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    filtered_tokens = [
        lemmatizer.lemmatize(token)
        for token in tokens
        if token not in stop_words
    ]

    return " ".join(filtered_tokens)

df["processed_review"] = df["clean_review"].apply(preprocess_text)

df[["clean_review", "processed_review"]].head()

# Remove Empty Reviews
df = df[df["processed_review"].str.strip() != ""]

# Final Dataset for Modeling
X = df["processed_review"]
y = df["condition"]

print(X.shape, y.shape)

# Word Frequency Analysis
from collections import Counter

all_words = " ".join(df["processed_review"]).split()
word_freq = Counter(all_words)

word_freq.most_common(20)

# word cloud visualization
from wordcloud import WordCloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)
plt.figure(figsize=(10,5))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

# tokenization and stopward analysis
import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

def tokenize(text):
    text = str(text).lower()
    tokens = re.findall(r'[a-z]+', text)
    return [t for t in tokens if t not in ENGLISH_STOP_WORDS]

df['tokens'] = df['review'].apply(tokenize)
df['tokens'].head()

# top keywords per sentiment
for s in df['sentiment'].unique():
    words = [w for tokens in df[df['sentiment']==s]['tokens'] for w in tokens]
    print(s, Counter(words).most_common(15))

# class imbalance summary
df.groupby(['condition','sentiment']).size().unstack(fill_value=0)

"""### MODEL BUILDING AND EVALUATION"""

# Import Required Libraries for Modeling
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    classification_report, confusion_matrix, roc_auc_score, roc_curve
)
from sklearn.preprocessing import label_binarize
import time

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"Training Set Size: {X_train.shape[0]}")
print(f"Test Set Size: {X_test.shape[0]}")
print("\nClass Distribution in Training Set:")
print(y_train.value_counts())
print("\nClass Distribution in Test Set:")
print(y_test.value_counts())

# TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

print(f"TF-IDF Matrix Shape: {X_train_tfidf.shape}")
print(f"Number of features: {len(tfidf.get_feature_names_out())}")

# Initialize Models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    'Naive Bayes': MultinomialNB(),
    'SVM': SVC(kernel='linear', probability=True, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
    'KNN': KNeighborsClassifier(n_neighbors=5)
}

print("Models initialized successfully!")

# Train and Evaluate All Models
results = {}

for name, model in models.items():
    print(f"\n{'='*50}")
    print(f"Training {name}...")

    # Train
    start_time = time.time()
    model.fit(X_train_tfidf, y_train)
    training_time = time.time() - start_time

    # Predict
    start_time = time.time()
    y_pred = model.predict(X_test_tfidf)
    prediction_time = time.time() - start_time

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')

    # Store results
    results[name] = {
        'model': model,
        'y_pred': y_pred,
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1_score': f1,
        'training_time': training_time,
        'prediction_time': prediction_time
    }

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print(f"Training Time: {training_time:.2f}s")
    print(f"Prediction Time: {prediction_time:.4f}s")

print("\n" + "="*50)
print("All models trained successfully!")

# Create Comparison DataFrame
comparison_df = pd.DataFrame({
    'Model': list(results.keys()),
    'Accuracy': [results[m]['accuracy'] for m in results],
    'Precision': [results[m]['precision'] for m in results],
    'Recall': [results[m]['recall'] for m in results],
    'F1-Score': [results[m]['f1_score'] for m in results],
    'Training Time (s)': [results[m]['training_time'] for m in results],
    'Prediction Time (s)': [results[m]['prediction_time'] for m in results]
})

comparison_df = comparison_df.sort_values('Accuracy', ascending=False)
comparison_df

# Visualize Model Comparison - Metrics
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']

for idx, (metric, color) in enumerate(zip(metrics, colors)):
    ax = axes[idx//2, idx%2]
    comparison_df.plot(
        x='Model', y=metric, kind='bar',
        ax=ax, color=color, legend=False
    )
    ax.set_title(f'{metric} Comparison', fontsize=14, fontweight='bold')
    ax.set_xlabel('')
    ax.set_ylabel(metric, fontsize=12)
    ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')
    ax.grid(axis='y', alpha=0.3)

    # Add value labels on bars
    for container in ax.containers:
        ax.bar_label(container, fmt='%.3f', padding=3)

plt.tight_layout()
plt.show()

# Visualize Training Time vs Accuracy
plt.figure(figsize=(10, 6))
plt.scatter(comparison_df['Training Time (s)'], comparison_df['Accuracy'],
            s=200, alpha=0.6, c=range(len(comparison_df)), cmap='viridis')

for idx, row in comparison_df.iterrows():
    plt.annotate(row['Model'],
                 (row['Training Time (s)'], row['Accuracy']),
                 xytext=(5, 5), textcoords='offset points', fontsize=10)

plt.xlabel('Training Time (seconds)', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.title('Model Training Time vs Accuracy', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Confusion Matrices for All Models
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.flatten()

for idx, (name, result) in enumerate(results.items()):
    cm = confusion_matrix(y_test, result['y_pred'])

    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                ax=axes[idx], cbar=True,
                xticklabels=np.unique(y_test),
                yticklabels=np.unique(y_test))

    axes[idx].set_title(f'{name}\nAccuracy: {result["accuracy"]:.4f}',
                        fontsize=12, fontweight='bold')
    axes[idx].set_xlabel('Predicted', fontsize=10)
    axes[idx].set_ylabel('Actual', fontsize=10)
    axes[idx].tick_params(axis='both', labelsize=8)

plt.tight_layout()
plt.show()

# Detailed Classification Reports
print("="*80)
print("DETAILED CLASSIFICATION REPORTS")
print("="*80)

for name, result in results.items():
    print(f"\n{'='*80}")
    print(f"{name}")
    print("="*80)
    print(classification_report(y_test, result['y_pred'], target_names=np.unique(y_test)))
    print()

# ROC-AUC Calculation for Multi-class Classification
# Binarize the labels for ROC-AUC calculation
classes = np.unique(y_test)
y_test_bin = label_binarize(y_test, classes=classes)
n_classes = y_test_bin.shape[1]

# Calculate ROC-AUC for each model
roc_auc_scores = {}

for name, result in results.items():
    model = result['model']

    # Get prediction probabilities
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test_tfidf)
    elif hasattr(model, "decision_function"):
        y_score = model.decision_function(X_test_tfidf)
    else:
        continue

    # Calculate ROC-AUC for each class and average
    try:
        roc_auc = roc_auc_score(y_test_bin, y_score, average='weighted', multi_class='ovr')
        roc_auc_scores[name] = roc_auc
        results[name]['roc_auc'] = roc_auc
    except:
        roc_auc_scores[name] = None

print("ROC-AUC Scores (Weighted Average):")
print("="*50)
for name, score in roc_auc_scores.items():
    if score is not None:
        print(f"{name:25s}: {score:.4f}")
    else:
        print(f"{name:25s}: N/A")

# ROC Curves for Best Model (Logistic Regression or best performer)
best_model_name = max(roc_auc_scores, key=lambda k: roc_auc_scores[k] if roc_auc_scores[k] else 0)
best_model = results[best_model_name]['model']

print(f"Plotting ROC Curves for: {best_model_name}")

# Get prediction probabilities
y_score = best_model.predict_proba(X_test_tfidf)

# Plot ROC curve for each class
plt.figure(figsize=(10, 8))

colors = ['blue', 'red', 'green']
for i, (color, class_name) in enumerate(zip(colors, classes)):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    roc_auc = roc_auc_score(y_test_bin[:, i], y_score[:, i])

    plt.plot(fpr, tpr, color=color, lw=2,
             label=f'{class_name} (AUC = {roc_auc:.3f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title(f'ROC Curves - {best_model_name}', fontsize=14, fontweight='bold')
plt.legend(loc="lower right", fontsize=10)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# Final Summary Table with ROC-AUC
final_comparison = comparison_df.copy()
final_comparison['ROC-AUC'] = final_comparison['Model'].map(lambda x: roc_auc_scores.get(x, None))

print("\n" + "="*100)
print("FINAL MODEL COMPARISON SUMMARY")
print("="*100)
print(final_comparison.to_string(index=False))
print("="*100)

# Identify best model
best_overall = final_comparison.loc[final_comparison['Accuracy'].idxmax()]
print(f"\nüèÜ BEST MODEL: {best_overall['Model']}")
print(f"   Accuracy: {best_overall['Accuracy']:.4f}")
print(f"   F1-Score: {best_overall['F1-Score']:.4f}")
if best_overall['ROC-AUC'] is not None:
    print(f"   ROC-AUC: {best_overall['ROC-AUC']:.4f}")

import pickle
import os

# Create directory
os.makedirs('models', exist_ok=True)

# The 'best_model' variable should exist from your training code
# The 'tfidf' variable should exist from your vectorization step

# Save model
with open('models/best_model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

# Save vectorizer
with open('models/tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)

print("‚úì Model files exported!")
print("Location: models/best_model.pkl, models/tfidf_vectorizer.pkl")

